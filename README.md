Autoencoder Implementation and Experimentation
This repository contains the implementation of an Autoencoder model, experimenting with different configurations to optimize its performance in reconstructing data. The project involves testing various activation functions, optimizers, and loss functions, as well as applying regularization techniques.

Table of Contents
Overview
Model Implementation
Experimentation and Results
Best Configuration
Analysis
Classifier Using Bottleneck Features
Conclusions
Overview
The purpose of this project is to implement an Autoencoder network and improve its reconstruction performance by:

Testing different activation functions, optimizers, and loss functions.
Applying Batch Normalization and Dropout to enhance generalization.
Training the model for 50 epochs and selecting the best configuration based on training and validation loss.
Model Implementation
The following components were tested during the Autoencoder training:

Activation Functions: tanh, sigmoid, and Leaky ReLU
Optimizers: Adam, SGD, and RMSprop with various learning rates
Loss Functions: MSE (Mean Squared Error) and binary_crossentropy
Regularization: Batch Normalization and Dropout
The model was trained approximately 40 times to find the optimal configuration. The best combination was selected based on the lowest loss during training.

Experimentation and Results
Tested Configurations:
We experimented with different combinations of:

3 Optimizers: Adam, SGD, and RMSprop
3 Activation Functions: tanh, sigmoid, and Leaky ReLU
2 Loss Functions: MSE and binary_crossentropy
Each model was trained for 50 epochs, and the results were evaluated by comparing the train and validation losses.

Best Configuration
The best performing combination is as follows:

Optimizer: Adam
Learning Rate: 0.01
Activation Function: Leaky ReLU
Loss Function: Mean Squared Error (MSE)
This configuration consistently achieved the lowest validation loss.

Key Reasons for Choosing Leaky ReLU and Adam:
Leaky ReLU: Solves the "dying ReLU" problem by introducing a small slope for negative values, ensuring all neurons remain active.
Adam Optimizer: Combines the benefits of RMSprop and Momentum, automatically adjusting the learning rate and stabilizing gradients for faster convergence.
Analysis
Loss and Accuracy Curves
Training Loss: Decreased steadily from ~2.5 to ~0.4 over the 50 epochs.
Validation Loss: Reduced quickly to ~0.3, suggesting good generalization.
Training Accuracy: Reached 88% by the end of the training.
Validation Accuracy: Peaked at 91%, demonstrating effective model training and minimal overfitting.
Reconstruction Results
The reconstructed images generated by the Autoencoder closely match the original input images, indicating that the model learned the underlying data representation effectively. Intermediate representations in the bottleneck layer (compressed feature space) were used to further analyze the model's performance.

Classifier Using Bottleneck Features
Using the 32-dimensional bottleneck features from the trained Autoencoder, we designed a simple classifier with two hidden layers. The classifier achieved:

Validation Accuracy: 92.8%
Conclusions
The optimal configuration for the Autoencoder model was found to be using:
Optimizer: Adam
Learning Rate: 0.01
Activation Function: Leaky ReLU
Loss Function: MSE
The model generalized well with minimal overfitting, as indicated by the close values of training and validation loss.
The classifier built on top of the bottleneck features performed well, achieving over 92% accuracy.

